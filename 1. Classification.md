# 1. Classification

## Big ideas

<ins>Definition</ins>: Given training set $(\textbf{x}_i, y_i)$, predict label $y$ for novel feature $x$. 

<ins>Evaluation</ins>:
- Performance is quantified by error and accuracy. 
- Further evaluation can be done by quantifying false negative and false positive rates.
- The class-confusion matrix summarizes errors for multiclass classification.

<ins>Overfitting</ins>: Classifiers performing better on training data than on test data. A possible solution is cross-validation.

<ins>Cross-validation</ins>:
1. Split data into training and validation sets.
2. Train classifier on training set.
3. Evaluate classifier on validation set.
4. Average error over all splits (a.k.a. folds).

## Nearest neighbors

<ins>Strategy</ins>: Find the labelled example $\textbf{x}_c$ that is closest to $\textbf{x}$, and report the class $y_c$ of that example.

<ins>Generalization</ins>: A $(k, l)$ nearest neighbor classiﬁer ﬁnds the $k$ example points closest to the point being considered, and classiﬁes this point with the class that has the highest number of votes (each of the $k$ neighbors votes for its own class), as long as this class has more than $l$ votes (otherwise, the point is classiﬁed as unknown). In practice, it's common to use an approximate nearest neighbor.

## Naive Bayes

Suppose the probability $p(y | \textbf{x})$ (typically unknown) is known and all classification errors have equal cost, then the following procedure provides the smallest possible error rate: 

<ins>Procedure</ins>: For a test example $\textbf{x}$, report the class $y$ that has the highest value of $p(y | \textbf{x})$. If the largest value is achieved by more than one class, choose randomly from that set of classes.

Assume features are conditionally independent on the class of the data item. That is:

$$p(\textbf{x} \mid y) = \prod_{j} p(x^{(j)} \mid y)$$

By Bayes' rule: 

$$\begin{align*}
p(y \mid \textbf{x}) &= \frac{p(\textbf{x} \mid y) p(y)}{p(\textbf{x})}\\
                     &= \frac{\prod_{j} p(x^{(j)} \mid y) p(y)}{p(\textbf{x})} \\
                     &\propto \prod_{j} p(x^{(j)} \mid y) p(y)
\end{align*}$$

To maximize $p(y \mid \textbf{x})$, we proceed as follows:
> Choose $y$ such that $\prod_{j} p(x^{(j)} \mid y) p(y)$ is largest

By the monotonicity of logarithms, the following rule is equivalent:
> Choose $y$ such that $(\sum_{j} \log p(x^{(j)} \mid y)) + \log p(y)$ is largest

To use this rule, we need models for $p(y)$ and for $p(x^{(j)} \mid y)$ for each $j$. 

1. To ﬁnd a model of $p(y)$, count the number of training examples in each class $y$, then divide by the number of classes.

2. To find a model for $p(x^{(j)} \mid y)$, we can use the following procedure:

<ins>Procedure</ins>: Cross-validation to choose a model

1. Randomly partition $\mathcal{D}$ into two parts: the training set $\mathcal{R}$ and the test set $\mathcal{E}$.

2. For each model of interest:

    Repeatedly

    – Randomly split $\mathcal{R}$ into two subsets: $\mathcal{R}_t$ and $\mathcal{V}$

    – Train the model on $\mathcal{R}_t$

    – Calculate the model's error using $\mathcal{V}$

    Calculate the average of the validation errors for that model.

3. Based on the average errors (and potentially other factors like speed), choose the model that performs best. 

4. Once a model is selected, estimate its error rate using the test set $\mathcal{E}$.

<ins>On missing data</ins>: Naive Bayes classifiers can deal with missing data easily by simply omitting it from the equation.